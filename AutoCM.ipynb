{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN4qoM5mG2wZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from osgeo import gdal\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFMkMTszG3aD"
      },
      "source": [
        "# AutoCM Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8HVWePSSC5i"
      },
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "num_per_pack = 10       # Number of images per pack\n",
        "num_bands = 4     # Number of bands (channels) per image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sr75687GwDl"
      },
      "outputs": [],
      "source": [
        "# Input pipeline from 3 spatiotemporal packs, each with a shape of [400, 400, num_per_pack*num_bands]\n",
        "def input_pipeline(filenames, batch_size, is_shuffle=True, is_train=True, is_repeat=True):\n",
        "    feature_description = {\n",
        "        'image_raw': tf.io.FixedLenFeature([400*400*num_per_pack*num_bands], dtype=tf.int64)\n",
        "    }\n",
        "\n",
        "    def _parse_function(example_proto):\n",
        "        feature_dict = tf.io.parse_single_example(example_proto, feature_description)\n",
        "\n",
        "        image = tf.reshape(feature_dict['image_raw'], [400, 400, num_per_pack*num_bands])\n",
        "        image = tf.cast(image, tf.float32)\n",
        "        image = image/10000\n",
        "\n",
        "        return image\n",
        "\n",
        "    def _aug(example1, example2, example3):\n",
        "        image = tf.concat([example1, example2, example3], axis=-1)\n",
        "        image = tf.image.rot90(image, tf.random.uniform([], 0, 5, dtype=tf.int32))\n",
        "        image = tf.image.random_flip_left_right(image)\n",
        "        image = tf.image.random_flip_up_down(image)\n",
        "\n",
        "        # pack1, pack2, pack3\n",
        "        return image[:, :, :num_per_pack*num_bands], \\\n",
        "               image[:, :, num_per_pack*num_bands:2*num_per_pack*num_bands], \\\n",
        "               image[:, :, 2*num_per_pack*num_bands:]\n",
        "\n",
        "    dataset = []\n",
        "    for fname in filenames:\n",
        "        print(fname)\n",
        "        pack = tf.data.TFRecordDataset(fname)\n",
        "        pack = pack.map(_parse_function)\n",
        "        dataset.append(pack)\n",
        "\n",
        "    dataset = tf.data.Dataset.zip(tuple(dataset))\n",
        "    if is_train:\n",
        "        dataset = dataset.map(_aug)\n",
        "\n",
        "    if is_repeat:\n",
        "        dataset = dataset.repeat()\n",
        "\n",
        "    if is_shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=100)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4iuwjPVHd48"
      },
      "outputs": [],
      "source": [
        "# downsampling step in the UNet\n",
        "def downsample(filters, size):\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(tf.keras.layers.Conv2D(filters, size, strides=1, padding='same',\n",
        "                                      activation=tf.nn.relu,\n",
        "                                      use_bias=True))\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    result.add(tf.keras.layers.Conv2D(filters, size, strides=1, padding='same',\n",
        "                                      activation=tf.nn.relu,\n",
        "                                      use_bias=True))\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                                      activation=tf.nn.relu,\n",
        "                                      use_bias=True))\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# upsampling step in the UNet\n",
        "def upsample(filters, size):\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(\n",
        "        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,padding='same',\n",
        "                                        activation=tf.nn.relu,\n",
        "                                        use_bias=True))\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    result.add(tf.keras.layers.Conv2D(filters, size, strides=1, padding='same',\n",
        "                                      activation=tf.nn.relu,\n",
        "                                      use_bias=True))\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    result.add(tf.keras.layers.Conv2D(filters, size, strides=1, padding='same',\n",
        "                                      activation=tf.nn.relu,\n",
        "                                      use_bias=True))\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# U-Net model with two heads, one for cloud masking, another for cloud brightness\n",
        "def unet(num_bands = num_bands):\n",
        "    down_stack = [\n",
        "        downsample(16, 3),\n",
        "        downsample(32, 3),\n",
        "        downsample(64, 3),\n",
        "    ]\n",
        "\n",
        "    up_stack = [\n",
        "        upsample(32, 3),\n",
        "        upsample(16, 3),\n",
        "    ]\n",
        "\n",
        "    # cloud mask head\n",
        "    last = tf.keras.Sequential()\n",
        "    last.add(tf.keras.layers.Conv2DTranspose(16, 3, strides=2, padding='same',\n",
        "                                            activation=tf.nn.relu))  # (bs, 128, 128, 16)\n",
        "    last.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    last.add(tf.keras.layers.Conv2D(16, 3, strides=1, padding='same',\n",
        "                                    activation=tf.nn.relu))\n",
        "    last.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    last.add(tf.keras.layers.Conv2D(1, 1, strides=1, padding='same',\n",
        "                                    activation=tf.sigmoid))\n",
        "\n",
        "    # cloud brightness head\n",
        "    last2 = tf.keras.Sequential()\n",
        "    last2.add(tf.keras.layers.Conv2D(16, 1, strides=1, padding='same',\n",
        "                                            activation=tf.nn.relu))\n",
        "    last2.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    last2.add(tf.keras.layers.Conv2D(16, 1, strides=1, padding='same',\n",
        "                                    activation=tf.nn.relu))\n",
        "    last2.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    last2.add(tf.keras.layers.Conv2D(4, 1, strides=1, padding='same',\n",
        "                                    activation=tf.sigmoid))\n",
        "\n",
        "\n",
        "    skips = []\n",
        "    concat = tf.keras.layers.Concatenate()\n",
        "\n",
        "    inputs = tf.keras.layers.Input(shape=[None, None, None])\n",
        "\n",
        "    bs = tf.shape(inputs)[0]\n",
        "    height = tf.shape(inputs)[1]\n",
        "    width = tf.shape(inputs)[2]\n",
        "\n",
        "    inputs_ = tf.reshape(inputs, [bs, height, width, -1, num_bands])\n",
        "    inputs_ = tf.transpose(inputs_, [0, 3, 1, 2, 4])\n",
        "    x = tf.reshape(inputs_, [-1, height, width, num_bands])\n",
        "\n",
        "    for down in down_stack:\n",
        "        x = down(x)\n",
        "        skips.append(x)\n",
        "\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        x = concat([x, skip])\n",
        "\n",
        "    mask = last(x) # cloud mask\n",
        "\n",
        "    x = tf.reduce_max(x, [1, 2], keepdims=True)\n",
        "    glight = last2(x) # cloud brightness\n",
        "\n",
        "    print(mask.shape, glight.shape)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=[mask, glight])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKLD45D5IFIq"
      },
      "outputs": [],
      "source": [
        "def discriminator(norm_type='batchnorm'):\n",
        "    inputs = tf.keras.layers.Input(shape=[None, None, 4])\n",
        "\n",
        "    down_stack = [\n",
        "        downsample(64, 3),\n",
        "        downsample(32, 3),\n",
        "        downsample(16, 3),\n",
        "    ]\n",
        "    x = inputs\n",
        "\n",
        "    for down in down_stack:\n",
        "        x = down(x)\n",
        "\n",
        "    x = tf.reduce_mean(x, [1, 2], keepdims=True)\n",
        "\n",
        "    last = tf.keras.layers.Conv2D(\n",
        "        16, 1, strides=1,\n",
        "        padding='same',\n",
        "        activation=tf.nn.relu)\n",
        "\n",
        "    x = last(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "# Simaese network for learning spatiotemporal similarity\n",
        "# input 1,2: cloud-free composite from two packs\n",
        "def Siamese_net():\n",
        "    input_1 = tf.keras.layers.Input(shape=[None, None, 4])\n",
        "    input_2 = tf.keras.layers.Input(shape=[None, None, 4])\n",
        "\n",
        "    d_model = discriminator()\n",
        "    diff = tf.square(d_model(input_1)-d_model(input_2))\n",
        "\n",
        "    last = tf.keras.Sequential()\n",
        "    last.add(tf.keras.layers.Conv2D(\n",
        "        16, 1, strides=1,\n",
        "        padding='same',\n",
        "        activation=tf.nn.relu))\n",
        "    last.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    last.add(tf.keras.layers.Conv2D(\n",
        "        1, 1, strides=1,\n",
        "        padding='same',\n",
        "        activation=tf.sigmoid))\n",
        "\n",
        "    final = last(diff)\n",
        "    final = tf.squeeze(final, axis=[1, 2])\n",
        "    return tf.keras.Model(inputs=[input_1, input_2], outputs=final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIJZLGcSKJDX",
        "outputId": "ac336712-cd5f-4fc3-dae9-204df2228174"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, None, None, 1) (None, 1, 1, 4)\n"
          ]
        }
      ],
      "source": [
        "u_class_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "d_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "u_model = unet()\n",
        "d_model = Siamese_net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SabvwOOrI9hJ"
      },
      "outputs": [],
      "source": [
        "# Calculate cloud-free composite\n",
        "def compose(img_pack, cloud_mask):\n",
        "    height = tf.shape(img_pack)[1]\n",
        "    width = tf.shape(img_pack)[2]\n",
        "\n",
        "    pack = tf.reshape(img_pack, [-1, height, width, num_per_pack, num_bands])\n",
        "    pack = tf.transpose(pack, [0, 3, 1, 2, 4])\n",
        "\n",
        "    pack_cloud = tf.reshape(1-cloud_mask, [-1, num_per_pack, height, width, 1])\n",
        "    denorm = tf.reduce_sum(pack_cloud, axis=1, keepdims=True)\n",
        "    composite = tf.reduce_sum(pack*pack_cloud/(denorm+1e-8), axis=1)\n",
        "\n",
        "    return composite\n",
        "\n",
        "\n",
        "def recons_loss(y_true, cloud_mask, composite, glight):\n",
        "    height = tf.shape(cloud_mask)[1]\n",
        "    width = tf.shape(cloud_mask)[2]\n",
        "\n",
        "    cloud_mask = tf.reshape(cloud_mask, [-1, num_per_pack, height, width, 1])\n",
        "    glight = tf.reshape(glight, [-1, num_per_pack, 1, 1, num_bands])\n",
        "\n",
        "    composite = tf.reshape(composite, [-1, 1, height, width, num_bands])\n",
        "\n",
        "    y_true = tf.reshape(y_true, [-1, height, width, num_per_pack, num_bands])\n",
        "    y_true = tf.transpose(y_true, [0, 3, 1, 2, 4])\n",
        "\n",
        "    recon = (1-cloud_mask)*composite + glight*cloud_mask\n",
        "\n",
        "    loss = tf.reduce_mean(tf.square(y_true-recon))\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Generate spatial overlapping packs\n",
        "def crop_generate(train_pack):\n",
        "    height = 200\n",
        "    width = 200\n",
        "    batch_size = tf.shape(train_pack)[0]\n",
        "\n",
        "    x_1 = 0\n",
        "    y_1 = 0\n",
        "    pack_crop1 = train_pack[:, y_1:y_1+height, x_1:x_1+width, :]\n",
        "\n",
        "    x_2 = np.array(random.choices(range(0, 201, 50), k=batch_size))\n",
        "    y_2 = np.array(random.choices(range(0, 201, 50), k=batch_size))\n",
        "    boxes = np.vstack([x_2, y_2, x_2+200, y_2+200]).transpose()/400\n",
        "    pack_crop2 = tf.image.crop_and_resize(train_pack, boxes=boxes, box_indices=[i for i in range(batch_size)], crop_size=[200, 200])\n",
        "\n",
        "    x_min = tf.math.maximum(x_1, x_2)\n",
        "    y_min = tf.math.maximum(y_1, y_2)\n",
        "    x_max = tf.math.minimum(x_1+200, x_2+200)\n",
        "    y_max = tf.math.minimum(y_1+200, y_2+200)\n",
        "    dx = tf.math.maximum(x_max - x_min, 0)\n",
        "    dy = tf.math.maximum(y_max - y_min, 0)\n",
        "    ratio = tf.cast(dx*dy, tf.float32)/40000.0\n",
        "\n",
        "    return pack_crop1, pack_crop2, ratio\n",
        "\n",
        "\n",
        "def train_step(train_pack1, train_pack2, train_pack3):\n",
        "    pack1_crop1, pack1_crop2, ratio_1 = crop_generate(train_pack1)\n",
        "    pack2_crop1, pack2_crop2, ratio_2 = crop_generate(train_pack2)\n",
        "    pack3_crop1, pack3_crop2, ratio_3 = crop_generate(train_pack3)\n",
        "\n",
        "    with tf.GradientTape() as gen_class_tape, tf.GradientTape() as disc_tape:\n",
        "        test_feature_11, glight11 = u_model(pack1_crop1, training=True)\n",
        "        test_feature_21, glight21 = u_model(pack2_crop1, training=True)\n",
        "\n",
        "        ################### reconstruction loss ####################\n",
        "        composite_11 = compose(pack1_crop1, test_feature_11)\n",
        "        loss_rec_11 = recons_loss(pack1_crop1, test_feature_11, composite_11, glight11)\n",
        "\n",
        "        composite_21 = compose(pack2_crop1, test_feature_21)\n",
        "        loss_rec_21 = recons_loss(pack2_crop1, test_feature_21, composite_21, glight21)\n",
        "\n",
        "        ################### Spatial similarity loss ####################\n",
        "        test_feature_12, glight12 = u_model(pack1_crop2, training=True)\n",
        "        test_feature_22, glight22 = u_model(pack2_crop2, training=True)\n",
        "\n",
        "        composite_12 = compose(pack1_crop2, test_feature_12)\n",
        "        loss_rec_12 = recons_loss(pack1_crop2, test_feature_12, composite_12, glight12)\n",
        "\n",
        "        composite_22 = compose(pack2_crop2, test_feature_22)\n",
        "        loss_rec_22 = recons_loss(pack2_crop2, test_feature_22, composite_22, glight22)\n",
        "\n",
        "        sim_1 = d_model([composite_21, composite_12], training=True)\n",
        "        sim_2 = d_model([composite_11, composite_22], training=True)\n",
        "\n",
        "        loss_spatial_sim = tf.reduce_mean(tf.square(sim_1-ratio_1)+tf.square(sim_2-ratio_2))/2\n",
        "\n",
        "        ################### Temporal similarity loss ####################\n",
        "        test_feature_31, glight31 = u_model(pack3_crop1, training=True)\n",
        "        composite_31 = compose(pack3_crop1, test_feature_31)\n",
        "        loss_rec_31 = recons_loss(pack3_crop1, test_feature_31, composite_31, glight31)\n",
        "\n",
        "        sim_1 = d_model([composite_31, composite_21], training=True)\n",
        "        sim_2 = d_model([composite_31, composite_11], training=True)\n",
        "\n",
        "        loss_temporal_sim = tf.reduce_mean(sim_2-sim_1)\n",
        "\n",
        "        loss_rec = (loss_rec_11 + loss_rec_21 + loss_rec_12 + loss_rec_22 + loss_rec_31)/5\n",
        "        loss = loss_rec+0.01*loss_spatial_sim+0.01*tf.nn.relu(loss_temporal_sim+0.2)\n",
        "\n",
        "        gradients_of_generator_from_class = gen_class_tape.gradient(loss, u_model.trainable_variables)\n",
        "        u_class_optimizer.apply_gradients(zip(gradients_of_generator_from_class, u_model.trainable_variables))\n",
        "\n",
        "        gradients_of_disc = disc_tape.gradient(loss, d_model.trainable_variables)\n",
        "        d_optimizer.apply_gradients(zip(gradients_of_disc, d_model.trainable_variables))\n",
        "\n",
        "        return loss, loss_rec, loss_spatial_sim, loss_temporal_sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lczWK-7QPHFZ"
      },
      "outputs": [],
      "source": [
        "# Training data: Image pack tfrecords list\n",
        "tfdir = '/content/drive/MyDrive/cloud_mapping_planet_world/tfdata'\n",
        "filenames1 = [os.path.join(tfdir, f) for f in os.listdir(tfdir) if f.endswith('1.tfrecords')]\n",
        "list.sort(filenames1)\n",
        "filenames2 = [os.path.join(tfdir, f) for f in os.listdir(tfdir) if f.endswith('2.tfrecords')]\n",
        "list.sort(filenames2)\n",
        "filenames3 = [os.path.join(tfdir, f) for f in os.listdir(tfdir) if f.endswith('3.tfrecords')]\n",
        "list.sort(filenames3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezlerJCQQ1ar"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    filenames = [filenames1, filenames2, filenames3]\n",
        "\n",
        "    # three packs will be zipped in the training pipeline\n",
        "    train_ds = input_pipeline(filenames, batch_size, is_repeat=False)\n",
        "    batchs = train_ds.batch(batch_size=batch_size)\n",
        "    for epoch in range(epoch):\n",
        "        loss_ = []\n",
        "        loss_rec_ = []\n",
        "        loss_sp_ = []\n",
        "        loss_tp_ = []\n",
        "\n",
        "        for train_pack1, train_pack2, train_pack3 in batchs:\n",
        "            loss, loss_rec, loss_sp, loss_tp = train_step(train_pack1, train_pack2, train_pack3)\n",
        "            loss_.append(loss)\n",
        "            loss_rec_.append(loss_rec)\n",
        "            loss_sp_.append(loss_sp)\n",
        "            loss_tp_.append(loss_tp)\n",
        "\n",
        "        print('train total loss over epoch %d: %.4f'% (epoch+1, np.mean(loss_)))\n",
        "        print('train rec loss over epoch %d: %.4f'% (epoch+1, np.mean(loss_rec_)))\n",
        "        print('train spatial loss over epoch %d: %.4f'% (epoch+1, np.mean(loss_sp_)))\n",
        "        print('train temporal loss over epoch %d: %.4f'% (epoch+1, np.mean(loss_tp_)))\n",
        "\n",
        "        if (epoch + 1) % 1 == 0:\n",
        "            u_model.save_weights('u_model_autocm')\n",
        "            d_model.save_weights('d_model_autocm')\n",
        "\n",
        "train(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifftVx4HURHY"
      },
      "source": [
        "# AutoCM Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC-FSf6dVNQU",
        "outputId": "58f5da0b-c7c7-46e1-854b-b2a6ad6cac30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, None, None, 1) (None, 1, 1, 4)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7d9981bbb2e0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "u_model_test = unet()\n",
        "u_model_test.load_weights('models/u_model_autocm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsuKJQFuUPxk"
      },
      "outputs": [],
      "source": [
        "def tif2array(input_file):\n",
        "    dataset = gdal.Open(input_file, gdal.GA_ReadOnly)\n",
        "    image = np.zeros((dataset.RasterYSize, dataset.RasterXSize, dataset.RasterCount),\n",
        "                     dtype=int)\n",
        "\n",
        "    for b in range(dataset.RasterCount):\n",
        "        band = dataset.GetRasterBand(b + 1)\n",
        "        image[:, :, b] = band.ReadAsArray()\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alsdiiVueOeQ"
      },
      "outputs": [],
      "source": [
        "# image stretch preprocessing\n",
        "def img_stretch(img, cut=2):\n",
        "    min_percent = cut   # Low percentile\n",
        "    max_percent = 100-cut  # High percentile\n",
        "    lo, hi = np.percentile(img[img>0], (min_percent, max_percent))\n",
        "    res_img = (img - lo) / (hi-lo)\n",
        "    res_img[res_img<0] = 0\n",
        "    res_img[res_img>1] = 1\n",
        "    return res_img\n",
        "\n",
        "# img should be reflectance\n",
        "def autocm_test(img, is_stretch=True, vis=True):\n",
        "    # image stretch preprocessing\n",
        "    if is_stretch:\n",
        "        img = img_stretch(img)\n",
        "    else:\n",
        "        img = img/10000\n",
        "\n",
        "    height, width, num_bands = tf.shape(img)\n",
        "    img = tf.reshape(img, [1, height, width, num_bands])\n",
        "\n",
        "    if height%8 != 0:\n",
        "        extend_height = (height//8 + 1)*8\n",
        "    else:\n",
        "        extend_height = height\n",
        "\n",
        "    if width%8 != 0:\n",
        "        extend_width = (width//8 + 1)*8\n",
        "    else:\n",
        "        extend_width = width\n",
        "    img = tf.image.resize_with_crop_or_pad(img, extend_height, extend_width)\n",
        "\n",
        "    pred, _ = u_model_test(img, training=False)\n",
        "    pred = tf.image.resize_with_crop_or_pad(pred, height, width)\n",
        "\n",
        "    if vis:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "\n",
        "        # Image and prediction are resampled to accelerate the display.\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img[0, ::4, ::4, 2::-1], vmin=0, vmax=1)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(pred[0, ::4, ::4, 0], vmin=0, vmax=1)\n",
        "\n",
        "    return pred.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nwjJF72VDMU",
        "outputId": "16ae6d9d-8d83-4dfc-b246-77ae4f3c5ef8"
      },
      "outputs": [],
      "source": [
        "# test image dataset\n",
        "data = '/content/drive/MyDrive/cloud_mapping_planet_test/dataset'\n",
        "imgs = [os.path.join(data, f) for f in os.listdir(data) if f.endswith('composite.tif')]\n",
        "list.sort(imgs)\n",
        "imgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3BNPujS7Xwd_",
        "outputId": "02064aff-6061-4d1e-fcda-8798cf008343"
      },
      "outputs": [],
      "source": [
        "for img_path in imgs:\n",
        "    # Test image size can be different from the training patch size\n",
        "    img = tif2array(img_path)\n",
        "    pred = autocm_test(img, is_stretch=True, vis=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
